{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib.request, json\n",
    "import underthesea\n",
    "import re\n",
    "import emoji\n",
    "import transformers\n",
    "import torch\n",
    "import joblib\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "phobert = transformers.AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "phobert.cuda()\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0-9]\", \" \", string)\n",
    "    string = give_emoji_free_text(string)\n",
    "    return string.strip()\n",
    "\n",
    "def give_emoji_free_text(text):\n",
    "    for i in text:\n",
    "        if i in emoji.UNICODE_EMOJI:\n",
    "            text = text.replace(i,'')\n",
    "    return text\n",
    "\n",
    "def remove_blank(s):\n",
    "    re = ''\n",
    "    for i in range(0,len(s)):\n",
    "        if s[i] == ' ':\n",
    "            if s[i-1] != ' ':\n",
    "                re = re + s[i]\n",
    "        else:\n",
    "            re = re + s[i]\n",
    "    return re\n",
    "\n",
    "def text_lowercase(string):\n",
    "    return string.lower()\n",
    "\n",
    "def tokenize(strings):    \n",
    "    return underthesea.word_tokenize(strings, format=\"text\")\n",
    "\n",
    "def remove_stopwords(strings):\n",
    "    strings = strings.split()\n",
    "    f = open('vietnamese-stopwords.txt', 'r',encoding=\"utf-8\")\n",
    "    stopwords = f.readlines()\n",
    "    stop_words = [s.replace(\"\\n\", '') for s in stopwords]\n",
    "    doc_words = []\n",
    "    \n",
    "    for word in strings:\n",
    "        if word not in stop_words:\n",
    "            doc_words.append(word)\n",
    "    doc_str = ' '.join(doc_words).strip()\n",
    "    return doc_str\n",
    "\n",
    "def text_preprocessing(strings):\n",
    "    temp = clean_str(strings)\n",
    "    temp = text_lowercase(temp)\n",
    "    temp = tokenize(temp)\n",
    "    return temp\n",
    "\n",
    "def text_preprocessing2(strings):\n",
    "    temp = clean_str(strings)\n",
    "    temp = text_lowercase(temp)\n",
    "    temp = remove_stopwords(temp)\n",
    "    return temp\n",
    "\n",
    "def removeEmptySentens(data):\n",
    "    data[0].replace('', np.nan, inplace=True)\n",
    "    data.dropna(subset=[0], inplace=True)\n",
    "\n",
    "def returnResult(a):\n",
    "    return len([i for i in a if i == 0]),len([i for i in a if i == 1])\n",
    "\n",
    "def runAPI(ids):\n",
    "    product_id = ids\n",
    "    print(product_id)\n",
    "    url = \"https://tiki.vn/api/v2/reviews?product_id=\"+product_id+\"&sort=score%7Casc,id%7Casc,stars%7Call&page=1&limit=300&include=comments\"\n",
    "    response = urllib.request.urlopen(url)\n",
    "    data = json.loads(response.read())\n",
    "    \n",
    "    data_crawl = []\n",
    "    list_comment = data.get('data')\n",
    "    for comment in list_comment:\n",
    "        if comment.get('content') != '':\n",
    "            data_crawl.append(comment.get('content'))\n",
    "    if len(data_crawl) == 0:\n",
    "        return 0,0\n",
    "#     data = []\n",
    "#     for i in data_crawl:\n",
    "#         data.append(text_preprocessing(i))\n",
    "#     data = pd.DataFrame(data)\n",
    "#     removeEmptySentens(data)\n",
    "    \n",
    "    temp_id = []\n",
    "    data_processbert = []\n",
    "    data_processtfidf = []\n",
    "    count = 0\n",
    "    for i in data_crawl:\n",
    "        data_processbert.append(text_preprocessing(i))\n",
    "        data_processtfidf.append(text_preprocessing2(i))\n",
    "        if data_processbert[count] == '' or data_processtfidf[count] == '':\n",
    "            temp_id.append(count)\n",
    "        count = count + 1\n",
    "    data = pd.DataFrame(data_processbert)\n",
    "    \n",
    "    count = 0\n",
    "    data = data.drop(temp_id)\n",
    "    for i in temp_id:\n",
    "        data_processtfidf.pop(i-count)\n",
    "        count = count + 1\n",
    "    removeEmptySentens(data)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(data_processtfidf).toarray()\n",
    "    \n",
    "    #encode lines\n",
    "    tokenized = data[0].apply((lambda x: tokenizer.encode(x, add_special_tokens = True)))\n",
    "    labels = np.zeros((len(data),))\n",
    "    \n",
    "    # get lenght max of tokenized\n",
    "    max_len = 254\n",
    "#     for i in tokenized.values:\n",
    "#         if len(i) > max_len:\n",
    "#             max_len = len(i)\n",
    "#     print('max len:', max_len)\n",
    "\n",
    "    # if lenght of tokenized not equal max_len , so padding value 0\n",
    "    padded = []\n",
    "    for i in tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            padded.append(i[0:max_len])\n",
    "        else:\n",
    "            padded.append(i + [0]*(max_len-len(i)))\n",
    "\n",
    "    padded = np.array(padded)\n",
    "    print('padded:', padded[1])\n",
    "    print('len padded:', np.array(padded).shape)\n",
    "\n",
    "    #get attention mask ( 0: not has word, 1: has word)\n",
    "    attention_mask = np.where(padded ==0, 0,1)\n",
    "\n",
    "    # Convert input to tensor\n",
    "    padded = torch.tensor(padded,dtype=int).cuda()\n",
    "    attention_mask = torch.tensor(attention_mask).cuda()\n",
    "    \n",
    "    labels_tensor = torch.tensor(labels).cuda()\n",
    "    \n",
    "    data = TensorDataset(padded, attention_mask,labels_tensor)\n",
    "    sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=32)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Train model\n",
    "    phobert.eval()\n",
    "    all_features = []\n",
    "    with torch.no_grad():\n",
    "        for index,batch in tqdm_notebook(enumerate(dataloader)):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            padded,attention_mask,_ = batch\n",
    "            last_hidden_states = phobert(padded, attention_mask =attention_mask)\n",
    "            all_features.extend(last_hidden_states[1].tolist().copy())\n",
    "\n",
    "    features = np.concatenate((all_features,X), axis=1)\n",
    "    \n",
    "    res = 5311\n",
    "    \n",
    "    tem = np.zeros((len(features),res-len(features[0])))\n",
    "    features = np.concatenate((features,tem), axis=1)\n",
    "    \n",
    "    model = joblib.load(r'save_model_nlp_svm.pkl')\n",
    "    result = model.predict(features)\n",
    "    return returnResult(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    print('using device: cuda')\n",
    "else:\n",
    "    print('using device: cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "import flask\n",
    "from flask import jsonify\n",
    "from IPython.display import clear_output\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Khai báo các route 1 cho API\n",
    "@app.route(\"/<ids>\", methods=[\"GET\"])\n",
    "# Khai báo hàm xử lý dữ liệu.\n",
    "def _hello_world(ids):\n",
    "    clear_output()\n",
    "    c0,c1 = runAPI(ids)\n",
    "    return jsonify(\n",
    "        tt=c0,\n",
    "        tc=c1\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"App run!\")\n",
    "    # Load model\n",
    "    app.run(debug=False, threaded=False,host='0.0.0.0', port=80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
