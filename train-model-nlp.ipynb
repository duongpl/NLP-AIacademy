{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn-deap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import underthesea\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn\n",
    "from evolutionary_search import EvolutionaryAlgorithmSearchCV\n",
    "from tqdm import tqdm_notebook\n",
    "import torch\n",
    "import transformers\n",
    "import joblib\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('../input/nlpdataver2/crawl-nlp-tiki-ver2.xlsx',header=None)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0-9]\", \" \", string)\n",
    "    string = give_emoji_free_text(string)\n",
    "    return string.strip()\n",
    "\n",
    "def give_emoji_free_text(text):\n",
    "    for i in text:\n",
    "        if i in emoji.UNICODE_EMOJI:\n",
    "            text = text.replace(i,'')\n",
    "    return text\n",
    "\n",
    "def remove_blank(s):\n",
    "    re = ''\n",
    "    for i in range(0,len(s)):\n",
    "        if s[i] == ' ':\n",
    "            if s[i-1] != ' ':\n",
    "                re = re + s[i]\n",
    "        else:\n",
    "            re = re + s[i]\n",
    "    return re\n",
    "\n",
    "def text_lowercase(string):\n",
    "    return string.lower()\n",
    "\n",
    "def tokenize(strings):    \n",
    "    return underthesea.word_tokenize(strings, format=\"text\")\n",
    "\n",
    "def remove_stopwords(strings):\n",
    "    strings = strings.split()\n",
    "    f = open('../input/vietnamesestopword/vietnamese-stopwords.txt', 'r',encoding=\"utf-8\")\n",
    "    stopwords = f.readlines()\n",
    "    stop_words = [s.replace(\"\\n\", '') for s in stopwords]\n",
    "    doc_words = []\n",
    "    \n",
    "    for word in strings:\n",
    "        if word not in stop_words:\n",
    "            doc_words.append(word)\n",
    "    doc_str = ' '.join(doc_words).strip()\n",
    "    return doc_str\n",
    "\n",
    "def text_preprocessing(strings):\n",
    "    temp = clean_str(strings)\n",
    "    temp = text_lowercase(temp)\n",
    "    temp = tokenize(temp)\n",
    "#     temp = remove_stopwords(temp)\n",
    "    return temp\n",
    "\n",
    "def text_preprocessing2(strings):\n",
    "    temp = clean_str(strings)\n",
    "    temp = text_lowercase(temp)\n",
    "    temp = remove_stopwords(temp)\n",
    "    return temp\n",
    "\n",
    "def removeEmptySentens(data):\n",
    "    data[0].replace('', np.nan, inplace=True)\n",
    "    data.dropna(subset=[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_id = []\n",
    "data_processbert = []\n",
    "data_processtfidf = []\n",
    "count = 0\n",
    "for i in data[0]:\n",
    "    data_processbert.append(text_preprocessing(i))\n",
    "    data_processtfidf.append(text_preprocessing2(i))\n",
    "    if data_processbert[count] == '' or data_processtfidf[count] == '':\n",
    "        print(count)\n",
    "        temp_id.append(count)\n",
    "    count = count + 1\n",
    "data[0] = pd.DataFrame(data_processbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    print('using device: cuda')\n",
    "else:\n",
    "    print('using device: cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "data = data.drop(temp_id)\n",
    "for i in temp_id:\n",
    "    data_processtfidf.pop(i-count)\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_processtfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeEmptySentens(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data_processtfidf).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phobert = transformers.AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode lines\n",
    "tokenized = data[0].apply((lambda x: tokenizer.encode(x, add_special_tokens = True)))\n",
    "print('encode',tokenized[1])\n",
    "# decode\n",
    "print('decode',tokenizer.decode(tokenized[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[1]\n",
    "print('labels shape:', labels.shape)\n",
    "\n",
    "# get lenght max of tokenized\n",
    "max_len = 254\n",
    "# for i in tokenized.values:\n",
    "#     if len(i) > max_len:\n",
    "#         max_len = len(i)\n",
    "print('max len:', max_len)\n",
    "\n",
    "# if lenght of tokenized not equal max_len , so padding value 0\n",
    "padded = []\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        padded.append(i[0:max_len])\n",
    "    else:\n",
    "        padded.append(i + [0]*(max_len-len(i)))\n",
    "# padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "\n",
    "padded = np.array(padded)\n",
    "print('padded:', padded[1])\n",
    "print('len padded:', np.array(padded).shape)\n",
    "\n",
    "#get attention mask ( 0: not has word, 1: has word)\n",
    "attention_mask = np.where(padded ==0, 0,1)\n",
    "\n",
    "# Convert input to tensor\n",
    "padded = torch.tensor(padded).cuda()\n",
    "attention_mask = torch.tensor(attention_mask).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tensor = torch.tensor(labels).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TensorDataset(padded, attention_mask, labels_tensor)\n",
    "sampler = SequentialSampler(data)\n",
    "dataloader = DataLoader(data, sampler=sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phobert.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train model\n",
    "phobert.eval()\n",
    "all_features = []\n",
    "with torch.no_grad():\n",
    "    for index,batch in tqdm_notebook(enumerate(dataloader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        padded,attention_mask,_ = batch\n",
    "        last_hidden_states = phobert(padded, attention_mask =attention_mask)\n",
    "        all_features.extend(last_hidden_states[1].tolist().copy())\n",
    "        print(index)\n",
    "        print(last_hidden_states[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(all_features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(all_features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate((all_features,X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels,test_size=0.1)\n",
    "\n",
    "cl = LogisticRegression()\n",
    "cl.fit(X_train, y_train)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(cl, 'save_model_nlp_logistic.pkl')\n",
    "print(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, cl.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_params = {\n",
    "#     'C': np.random.uniform(0,50,1000),\n",
    "#     \"kernel\":['linear','poly','rbf','sigmoid']\n",
    "# }\n",
    "# clf = sklearn.svm.SVC(gamma='scale')\n",
    "# ga1 = EvolutionaryAlgorithmSearchCV(estimator=clf,\n",
    "#                                    params=rf_params,\n",
    "#                                    scoring=\"accuracy\",\n",
    "#                                    cv=3,\n",
    "#                                    verbose=1,\n",
    "#                                    population_size=10,\n",
    "#                                    gene_mutation_prob=0.10,\n",
    "#                                    gene_crossover_prob=0.5,\n",
    "#                                    tournament_size=3,\n",
    "#                                    generations_number=5,\n",
    "#                                    n_jobs=1)\n",
    "# ga1.fit(X_train, y_train)\n",
    "# print(ga1.best_params_)\n",
    "# print(\"Accuracy:\"+ str(ga1.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = sklearn.svm.SVC(C = 5.80912162343139,kernel = 'rbf')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(cl, 'save_model_nlp_svm.pkl')\n",
    "print(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, svm.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the hyperparameter configuration space\n",
    "# rf_params = {\n",
    "#     'n_estimators': [10, 20, 30],\n",
    "#     #'max_features': ['sqrt',0.5],\n",
    "#     'max_depth': [15,20,30,50],\n",
    "#     #'min_samples_leaf': [1,2,4,8],\n",
    "#     #\"bootstrap\":[True,False],\n",
    "#     \"criterion\":['gini','entropy']\n",
    "# }\n",
    "# clf = RandomForestClassifier(random_state=0)\n",
    "# grid = GridSearchCV(clf, rf_params, cv=3, scoring='accuracy')\n",
    "# grid.fit(X_test, y_test)\n",
    "# print(grid.best_params_)\n",
    "# print(\"Accuracy:\"+ str(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf= RandomForestClassifier(n_estimators = 20, criterion='entropy',max_depth=15)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(rf, 'save_model_nlp_rf.pkl')\n",
    "print(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, rf.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
