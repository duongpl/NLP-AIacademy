{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://tiki.vn/api/v2/reviews?product_id=22095583&sort=score%7Casc,id%7Casc,stars%7Call&page=1&limit=300&include=comments\"\n",
    "# response = urllib.request.urlopen(url)\n",
    "# data = json.loads(response.read())\n",
    "    \n",
    "# list_comment = data.get('data')\n",
    "# data = []\n",
    "# for comment in list_comment:\n",
    "#     if comment.get('content') != '':\n",
    "#         print(comment.get('content'))\n",
    "#         data.append(comment.get('content'))\n",
    "# print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in data:\n",
    "#     print(text_preprocessing(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib.request, json\n",
    "import underthesea\n",
    "import re\n",
    "import emoji\n",
    "import transformers\n",
    "import torch\n",
    "import joblib\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "phobert = transformers.AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0-9]\", \" \", string)\n",
    "    string = give_emoji_free_text(string)\n",
    "    return string.strip()\n",
    "\n",
    "def give_emoji_free_text(text):\n",
    "    for i in text:\n",
    "        if i in emoji.UNICODE_EMOJI:\n",
    "            text = text.replace(i,'')\n",
    "    return text\n",
    "\n",
    "def remove_blank(s):\n",
    "    re = ''\n",
    "    for i in range(0,len(s)):\n",
    "        if s[i] == ' ':\n",
    "            if s[i-1] != ' ':\n",
    "                re = re + s[i]\n",
    "        else:\n",
    "            re = re + s[i]\n",
    "    return re\n",
    "\n",
    "def text_lowercase(string):\n",
    "    return string.lower()\n",
    "\n",
    "def tokenize(strings):    \n",
    "    return underthesea.word_tokenize(strings, format=\"text\")\n",
    "\n",
    "def remove_stopwords(strings):\n",
    "    strings = strings.split()\n",
    "    f = open('vietnamese-stopwords.txt', 'r',encoding=\"utf-8\")\n",
    "    stopwords = f.readlines()\n",
    "    stop_words = [s.replace(\"\\n\", '') for s in stopwords]\n",
    "    doc_words = []\n",
    "    \n",
    "    for word in strings:\n",
    "        if word not in stop_words:\n",
    "            doc_words.append(word)\n",
    "    doc_str = ' '.join(doc_words).strip()\n",
    "    return doc_str\n",
    "\n",
    "def text_preprocessing(strings):\n",
    "    temp = clean_str(strings)\n",
    "    temp = text_lowercase(temp)\n",
    "    temp = tokenize(temp)\n",
    "    return temp\n",
    "\n",
    "def text_preprocessing2(strings):\n",
    "    temp = clean_str(strings)\n",
    "    temp = text_lowercase(temp)\n",
    "    temp = remove_stopwords(temp)\n",
    "    return temp\n",
    "\n",
    "def removeEmptySentens(data):\n",
    "    data[0].replace('', np.nan, inplace=True)\n",
    "    data.dropna(subset=[0], inplace=True)\n",
    "\n",
    "def returnResult(a):\n",
    "    return len([i for i in a if i == 0]),len([i for i in a if i == 1])\n",
    "\n",
    "def runAPI(ids):\n",
    "    product_id = ids\n",
    "    print(product_id)\n",
    "    url = \"https://tiki.vn/api/v2/reviews?product_id=\"+product_id+\"&sort=score%7Casc,id%7Casc,stars%7Call&page=1&limit=300&include=comments\"\n",
    "    response = urllib.request.urlopen(url)\n",
    "    data = json.loads(response.read())\n",
    "    \n",
    "    data_crawl = []\n",
    "    list_comment = data.get('data')\n",
    "    for comment in list_comment:\n",
    "        if comment.get('content') != '':\n",
    "            data_crawl.append(comment.get('content'))\n",
    "    if len(data_crawl) == 0:\n",
    "        return 0,0\n",
    "#     data = []\n",
    "#     for i in data_crawl:\n",
    "#         data.append(text_preprocessing(i))\n",
    "#     data = pd.DataFrame(data)\n",
    "#     removeEmptySentens(data)\n",
    "    \n",
    "    temp_id = []\n",
    "    data_processbert = []\n",
    "    data_processtfidf = []\n",
    "    count = 0\n",
    "    for i in data_crawl:\n",
    "        data_processbert.append(text_preprocessing(i))\n",
    "        data_processtfidf.append(text_preprocessing2(i))\n",
    "        if data_processbert[count] == '' or data_processtfidf[count] == '':\n",
    "            temp_id.append(count)\n",
    "        count = count + 1\n",
    "    data = pd.DataFrame(data_processbert)\n",
    "    \n",
    "    count = 0\n",
    "    data = data.drop(temp_id)\n",
    "    for i in temp_id:\n",
    "        data_processtfidf.pop(i-count)\n",
    "        count = count + 1\n",
    "    removeEmptySentens(data)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(data_processtfidf).toarray()\n",
    "    \n",
    "    #encode lines\n",
    "    tokenized = data[0].apply((lambda x: tokenizer.encode(x, add_special_tokens = True)))\n",
    "    labels = np.zeros((len(data),))\n",
    "    \n",
    "    # get lenght max of tokenized\n",
    "    max_len = 254\n",
    "#     for i in tokenized.values:\n",
    "#         if len(i) > max_len:\n",
    "#             max_len = len(i)\n",
    "#     print('max len:', max_len)\n",
    "\n",
    "    # if lenght of tokenized not equal max_len , so padding value 0\n",
    "    padded = []\n",
    "    for i in tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            padded.append(i[0:max_len])\n",
    "        else:\n",
    "            padded.append(i + [0]*(max_len-len(i)))\n",
    "\n",
    "    padded = np.array(padded)\n",
    "    print('padded:', padded[1])\n",
    "    print('len padded:', np.array(padded).shape)\n",
    "\n",
    "    #get attention mask ( 0: not has word, 1: has word)\n",
    "    attention_mask = np.where(padded ==0, 0,1)\n",
    "\n",
    "    # Convert input to tensor\n",
    "    padded = torch.tensor(padded,dtype=int)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "    \n",
    "    labels_tensor = torch.tensor(labels).cuda()\n",
    "    \n",
    "    data = TensorDataset(padded, attention_mask,labels_tensor)\n",
    "    sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=32)\n",
    "\n",
    "    # Train model\n",
    "    phobert.eval()\n",
    "    all_features = []\n",
    "    with torch.no_grad():\n",
    "        for index,batch in tqdm_notebook(enumerate(dataloader)):\n",
    "            batch = tuple(t.to('cpu') for t in batch)\n",
    "            padded,attention_mask,_ = batch\n",
    "            last_hidden_states = phobert(padded, attention_mask =attention_mask)\n",
    "            all_features.extend(last_hidden_states[1].tolist().copy())\n",
    "\n",
    "    features = np.concatenate((all_features,X), axis=1)\n",
    "    \n",
    "    res = 5311\n",
    "    \n",
    "    tem = np.zeros((len(features),res-len(features[0])))\n",
    "    features = np.concatenate((features,tem), axis=1)\n",
    "    \n",
    "    model = joblib.load(r'save_model_nlp_svm.pkl')\n",
    "    result = model.predict(features)\n",
    "    return returnResult(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2946305\n",
      "padded: [    0   574   119   287   130  1397   265  8179    17  3289    66    11\n",
      "  3631    99   459  9059   145   227  2275  2289 16430    77    26  1330\n",
      "  2275    45 29041     6  6480    51  1800     8    17   370    30   145\n",
      "    12  2275 16430  1287   281   164   306  2792   588 16430  1329   103\n",
      "    77  8480   101   127   102  1895  1397    74   117   281   167  1329\n",
      " 13304     8    91    17    10 18446   441  3941    77   359  5831    11\n",
      "   219  3941     2     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n",
      "len padded: (68, 254)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-0cc83aba3345>:142: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for index,batch in tqdm_notebook(enumerate(dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9fd8466289405baac4c29c4b14a0ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\duongpl\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.23.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "172.16.0.242 - - [27/Jan/2021 15:46:26] \"\u001b[37mGET /2946305 HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request\n",
    "import flask\n",
    "from flask import jsonify\n",
    "from IPython.display import clear_output\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Khai báo các route 1 cho API\n",
    "@app.route(\"/<ids>\", methods=[\"GET\"])\n",
    "# Khai báo hàm xử lý dữ liệu.\n",
    "def _hello_world(ids):\n",
    "    clear_output()\n",
    "    c0,c1 = runAPI(ids)\n",
    "    return jsonify(\n",
    "        tt=c0,\n",
    "        tc=c1\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tprint(\"App run!\")\n",
    "\t# Load model\n",
    "\tapp.run(debug=False, threaded=False,host='0.0.0.0', port=80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
